# Spark
–í—ã —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –∂–∏–∑–Ω–µ—Å–ø–æ—Å–æ–±–Ω—ã–π pipeline:  –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–±—ã—Ç–∏–π ‚Üí Kafka ‚Üí Spark Streaming ‚Üí –ê–Ω–∞–ª–∏—Ç–∏–∫–∞
–î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã **Spark Streaming** —Å –ø–æ—Ç–æ–∫–æ–º –¥–∞–Ω–Ω—ã—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–º Python-—Å–∫—Ä–∏–ø—Ç–æ–º –∏ –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã–º —á–µ—Ä–µ–∑ **Kafka**, –≤—ã —É–∂–µ —Å–æ–∑–¥–∞–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∏ —Ä–∞–±–æ—á–∏–π —Å—Ç–µ–∫:

---

### ‚úÖ –°—Ç–µ–∫:
1. **Docker + Kafka/ZooKeeper**
2. **Python Producer** ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç JSON-—Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ç–æ–ø–∏–∫ Kafka
3. **Spark Streaming App** ‚Äî —á–∏—Ç–∞–µ—Ç –∏–∑ Kafka, –ø–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã–≤–æ–¥–∏—Ç –≤ –∫–æ–Ω—Å–æ–ª—å

---

## üß† –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç?

### 1. **–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (Docker + Kafka)**

```yaml
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
```

–≠—Ç–æ—Ç `docker-compose.yml` –∑–∞–ø—É—Å–∫–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä Kafka –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.

**–ó–∞–ø—É—Å–∫:**
```bash
docker compose up -d
```

---

### 2. **Python Producer**

```python
from kafka import KafkaProducer
import json
import time
import random

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

while True:
    data = {"value": random.randint(1, 100)}
    print("Sending:", data)
    producer.send("test-topic", value=data)
    time.sleep(1)
```

- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω–æ–µ —á–∏—Å–ª–æ –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É.
- –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –µ–≥–æ –≤ —Ç–æ–ø–∏–∫ `test-topic`.

---

### 3. **Spark Streaming Consumer**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, IntegerType

spark = SparkSession.builder \
    .appName("KafkaSparkStreaming") \
    .master("local[*]") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1") \
    .getOrCreate()

schema = StructType().add("value", IntegerType())

df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "test-topic") \
    .load()

parsed = df.selectExpr("CAST(value AS STRING) as json_str") \
    .select(from_json(col("json_str"), schema).alias("data")) \
    .select("data.value")

query = parsed.writeStream \
    .format("console") \
    .outputMode("append") \
    .start()

query.awaitTermination()
```

#### –ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥:
1. –ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Kafka –∫–∞–∫ consumer.
2. –ß–∏—Ç–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ —Ç–æ–ø–∏–∫–∞ `test-topic`.
3. –ü–∞—Ä—Å–∏—Ç JSON-—Å—Ç—Ä–æ–∫—É –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
4. –í—ã–≤–æ–¥–∏—Ç –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Å–æ–ª—å –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.

---

## üöÄ –ó–∞–ø—É—Å–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:

1. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã:**
   ```bash
   docker compose up -d
   ```

2. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ Python Producer:**
   ```bash
   python producer.py
   ```

3. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ Spark Streaming –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:**
   ```bash
   spark-submit spark_streaming.py
   ```

---

## üí° –ß—Ç–æ –¥–∞–ª—å—à–µ? –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏

–í—ã —É–∂–µ –ø–æ–Ω–∏–º–∞–µ—Ç–µ, —á—Ç–æ –º–æ–∂–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –Ω–µ –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–∞, –∞ —Å–ª–æ–∂–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ–∫–∏, —Å–æ–±—ã—Ç–∏—è –∫–ª–∏–∫–æ–≤, –º–µ—Ç—Ä–∏–∫–∏ –¥–∞—Ç—á–∏–∫–æ–≤ –∏ —Ç.–¥.). –í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —É—Å–ª–æ–∂–Ω–µ–Ω–∏—è:

---

### üî¢ –ü—Ä–∏–º–µ—Ä —É—Å–ª–æ–∂–Ω–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏:

> –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, —á—Ç–æ –≤—ã –ø–æ–ª—É—á–∞–µ—Ç–µ **—á–µ–∫–∏** –∏–∑ –º–∞–≥–∞–∑–∏–Ω–æ–≤, –∫–∞–∂–¥—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç:
- ID –º–∞–≥–∞–∑–∏–Ω–∞
- ID –∫–∞—Å—Å–∏—Ä–∞
- –í—Ä–µ–º—è
- –¢–æ–≤–∞—Ä—ã (–Ω–∞–∑–≤–∞–Ω–∏–µ, –∫–∞—Ç–µ–≥–æ—Ä–∏—è, —Ü–µ–Ω–∞, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)

–í–∞–º –Ω—É–∂–Ω–æ:
1. **–¢–æ–ø-5 –º–∞–≥–∞–∑–∏–Ω–æ–≤ –ø–æ –≤—ã—Ä—É—á–∫–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–µ–∫—É–Ω–¥**
2. **–ß–µ–∫–∏ –¥–æ—Ä–æ–∂–µ 500 —Ä—É–±–ª–µ–π**
3. **–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º**

---

### üõ†Ô∏è –ö–∞–∫ —ç—Ç–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å:

1. –ò–∑–º–µ–Ω–∏—Ç—å Producer, —á—Ç–æ–±—ã –æ–Ω –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª —á–µ–∫–∏ (–≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç—ã—Ö —á–∏—Å–µ–ª).
2. –í Spark –∏–∑–º–µ–Ω–∏—Ç—å —Å—Ö–µ–º—É –¥–∞–Ω–Ω—ã—Ö (`StructType`) –ø–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —á–µ–∫–∞.
3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–∫–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é (`window(...)`) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –ø—Ä–æ–º–µ–∂—É—Ç–∫–∞–º.
4. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ –¥–µ–ª–∞—Ç—å –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ (sum, count –∏ —Ç.–¥.).

---

## üìå –ü—Ä–∏–º–µ—Ä —É–ª—É—á—à–µ–Ω–∏–π –≤ Spark Streaming

–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–∫–æ–Ω–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏:

```python
from pyspark.sql.functions import window

# –ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è flat_df
windowed_df = batch_df.withWatermark("timestamp", "10 seconds").groupBy(
    window(col("timestamp"), "10 seconds"),
    col("store_name")
).agg(spark_sum("item_total").alias("revenue"))
```

---

## ‚ú® –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–í—ã —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –∂–∏–∑–Ω–µ—Å–ø–æ—Å–æ–±–Ω—ã–π pipeline:

**–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–±—ã—Ç–∏–π ‚Üí Kafka ‚Üí Spark Streaming ‚Üí –ê–Ω–∞–ª–∏—Ç–∏–∫–∞**

–≠—Ç–æ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö —Å–∏—Å—Ç–µ–º:  
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–¥–∞–∂ –≤ —Ä–∏—Ç–µ–π–ª–µ  
- –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π  
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏  
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ IoT —É—Å—Ç—Ä–æ–π—Å—Ç–≤  

–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ, —è –º–æ–≥—É –ø–æ–º–æ—á—å –≤–∞–º –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–µ–∫—Ç –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –±–∏–∑–Ω–µ—Å-–∑–∞–¥–∞—á—É –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ ClickHouse, PostgreSQL –∏–ª–∏ Kafka.
